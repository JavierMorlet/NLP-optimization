{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3396111",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43aa9f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import fsolve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e2da88",
   "metadata": {},
   "source": [
    "# Solve Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8aa5b7",
   "metadata": {},
   "source": [
    "The Non linear programming problem to solve is\n",
    "\n",
    "\n",
    "$ \\max \\left[ (x_{1}^{0.6} + x_{2}^{0.6} + 2 \\, x_{1}) - 4 \\, x_{3} \\right] $\n",
    "\n",
    "st.\n",
    "\n",
    "$ -3 \\, x_{1} + x_{2}^2 - 3 \\, x_{3} = 0 $\n",
    "\n",
    "$ x_{1} + 9 \\, \\sqrt{x_{2}} <= 25 $\n",
    "\n",
    "$ x_{1} + 9 \\, \\sqrt{x_{3}} <= 25 $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e717f5f0",
   "metadata": {},
   "source": [
    "The problem is divided in three parts, the equality constrains, the inequality constrains, and the objective function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a1faf6",
   "metadata": {},
   "source": [
    "## Equality Constrains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bbdda5",
   "metadata": {},
   "source": [
    "The equality constrains consists of solving a system of equations. In this case, we only have one equation and three variables, so we solve for $x_{3}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c867f2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Equations(X, *args):\n",
    "    \n",
    "    x1 = X[0]\n",
    "    x2 = args[0]\n",
    "    x3 = args[1]\n",
    "    \n",
    "    Func = np.empty((1))\n",
    "    \n",
    "    Func[0] = -3*x1 + x2**2 - 3*x3\n",
    "    \n",
    "    return Func\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a9d4d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLP_eq(X):\n",
    "    \n",
    "    x1 = X[0]\n",
    "    x2 = X[1]\n",
    "    x3_0 = X[2]\n",
    "    \n",
    "    x3 = fsolve(Equations, (x3_0), args=(x1, x2))[0]\n",
    "    \n",
    "    return [x1, x2, x3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b5e577",
   "metadata": {},
   "source": [
    "## Inequality Constrains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c09af82",
   "metadata": {},
   "source": [
    "Once the system of equality constrains is solved, the inequality constrains are reviewed to verify they all check. If all check, the varaible feas equals one, and the other case is set equal to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2af3530c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLP_in(X):\n",
    "    \n",
    "    x1 = X[0]\n",
    "    x2 = X[1]\n",
    "    x3 = X[2]\n",
    "\n",
    "    Res = np.empty((8))\n",
    "    \n",
    "    Res[0] = x1 + 9*np.sqrt(x3) <= 25\n",
    "    Res[1] = x1 + 9*np.sqrt(x2) <= 25\n",
    "\n",
    "    Res[2] = x1 >= 0\n",
    "    Res[3] = x2 >= 0\n",
    "    Res[4] = x3 >= 0\n",
    "\n",
    "    Res[5] = x1 <= 10\n",
    "    Res[6] = x2 <= 10\n",
    "    Res[7] = x3 <= 10\n",
    "\n",
    "    if Res[0] and Res[1] and Res[2] and Res[3] and Res[4] and Res[5] and Res[6] and Res[7]:\n",
    "        feas = 1\n",
    "    else:\n",
    "        feas = 0\n",
    "        \n",
    "    return feas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19ee171",
   "metadata": {},
   "source": [
    "## Objective function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1c814f",
   "metadata": {},
   "source": [
    "This function returns the value of the objective function. It returns the minimum value that float can represent if there is no real/determinate solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d36abe5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLP_obj(X):\n",
    "    \n",
    "    x1 = X[0]\n",
    "    x2 = X[1]\n",
    "    x3 = X[2]\n",
    "    \n",
    "    z0 = (x1**0.6 + x2**0.6 + 2*x1) - 4*x3\n",
    "    \n",
    "    if isinstance(z0, complex) or math.isnan(z0):\n",
    "        z = sys.float_info.min\n",
    "    else:\n",
    "        z = z0\n",
    "        \n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13599931",
   "metadata": {},
   "source": [
    "## Integrate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060cb9af",
   "metadata": {},
   "source": [
    "Here, we put all the parts of the NLP problem in one function. The function takes the variables as inputs, and returns the varables solved, the feasibility of the problem, and the objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "edb31e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLP(X_0):\n",
    "    \n",
    "    x1_0 = X_0[0]\n",
    "    x2_0 = X_0[1]\n",
    "    x3_0 = X_0[2]\n",
    "    \n",
    "    X = NLP_eq(X_0)\n",
    "    feas = NLP_in(X)\n",
    "    z =  NLP_obj(X)\n",
    "    \n",
    "    return feas, z, X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8031d4",
   "metadata": {},
   "source": [
    "# Create Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e7507a",
   "metadata": {},
   "source": [
    "The database is created, by solving iteratevly the NLP problem given some variables sampling. The results are stored in a numpy array called Data. The first colum stores the value of the feasibility of the problem, the second the value of the objective function, and the rest the value of the continuos variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8cefc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_Data(X_0, X_low, X_up, points):\n",
    "    \n",
    "    x1_0 = X_0[0]\n",
    "    x2_0 = X_0[1]\n",
    "    x3_0 = X_0[2]\n",
    "\n",
    "    x1_low = X_low[0]\n",
    "    x2_low = X_low[1]\n",
    "    x3_low = X_low[2]\n",
    "\n",
    "    x1_up = X_up[0]\n",
    "    x2_up = X_up[1]\n",
    "    x3_up = X_up[2]\n",
    "    \n",
    "    Data = []\n",
    "    \n",
    "    for x1_0 in np.linspace(X_low[0], X_up[0], points):\n",
    "        for x2_0 in np.linspace(X_low[1], X_up[1], points):\n",
    "            feas, z, X = NLP([x1_0, x2_0, x3_0])\n",
    "            x3_0 = X[-1]\n",
    "            Data.append(np.concatenate((np.array([feas, z]), np.array(X))).tolist())\n",
    "\n",
    "    return np.array(Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29c99b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r8/39l5fwnj4wg5s8cbqcdmdj2h0000gn/T/ipykernel_4918/4233216247.py:9: RuntimeWarning: invalid value encountered in sqrt\n",
      "  Res[0] = x1 + 9*np.sqrt(x3) <= 25\n"
     ]
    }
   ],
   "source": [
    "n_vars = 3\n",
    "\n",
    "X_0 = [0, 0, 0]\n",
    "X_low = [0, 0, 0]\n",
    "X_up = [10, 10, 10]\n",
    "points = 15\n",
    "Data = Create_Data(X_0, X_low, X_up, points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a75b93",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba3821d",
   "metadata": {},
   "source": [
    "In this section, the database is prepocesed. The fisrt colum is stored as y_clas, the second as z_clas, and the rest as Vars_clas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2f06bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prep = Data[:, 0]\n",
    "z_prep = Data[:, 1]\n",
    "Vars_prep = Data[:, 2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9680219",
   "metadata": {},
   "source": [
    "We use two scalers, MinMaxScaler for the objective function, and StandardScaler for the variables. Moreover, the variables are reduced using Principal Component Analysis, altough other techniques will be explored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c5fa720e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_minmax = MinMaxScaler()\n",
    "scaler_std = StandardScaler()\n",
    "all_classes = np.unique(y_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b57fd11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Vars_scaled = scaler_std.fit_transform(Vars_prep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23197c86",
   "metadata": {},
   "source": [
    "## Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4f3429f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_red = PCA(n_components=0.95)\n",
    "Vars_red = clf_red.fit_transform(Vars_scaled)\n",
    "dim_pca = clf_red.n_components_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc9d5d7",
   "metadata": {},
   "source": [
    "Once the variables are scaled, we create three databases\n",
    "\n",
    "* Clasification Database (Data_clas): The same as Data, but with the variables reduced and scaled\n",
    "* Regresion Database (Data_reg): Take the points where there is solution of the NLP problem, and scale the objective function (z_reg)\n",
    "* Clustering Database (Data_clust): The Regresion Database is split in two, the Data_feas where the NLP inequalities are met, and Data_infeas where the inequality constraints do not hold. \n",
    "\n",
    "Also, two other databases are created, Data_infeas with all the points where there is no solution or the inequalities does not hold, and Data_feas whith all the feasible solutions.\n",
    "We also know the solution, so we store it as Data_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42790db8",
   "metadata": {},
   "source": [
    "## Clasification Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "14a3c540",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_clas = np.concatenate((y_prep.reshape(-1,1), z_prep.reshape(-1,1), Vars_red), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "45e16506",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_clas = Data_clas[:, 0]\n",
    "z_clas = Data_clas[:, 1]\n",
    "Vars_clas = Data_clas[:, 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4037ad94",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_clas, X_test_clas, y_train_clas, y_test_clas = train_test_split(Vars_clas, y_clas, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0e04c53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_infeas = Data_clas[Data_clas[:, 0] == 0][:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a0cf0d",
   "metadata": {},
   "source": [
    "## Regresion Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5445856a",
   "metadata": {},
   "source": [
    "First, the datas where there in not solution are segregated. Then, the objective function (z_reg) is scaled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5d167a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_intractable = Data_clas[Data_clas[:, 1] != sys.float_info.min]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a45d9a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_reg = Data_intractable[:, 0]\n",
    "z_reg = Data_intractable[:, 1]\n",
    "Vars_reg = Data_intractable[:, 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f6d1c860",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_scaled = scaler_minmax.fit_transform(np.array(z_reg).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2b2446c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_reg = np.concatenate((y_reg.reshape(-1,1), z_scaled.reshape(-1,1), Vars_reg), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c8196daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reg, X_test_reg, z_train_reg, z_test_reg = train_test_split(Vars_reg, z_scaled, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da1bb7f",
   "metadata": {},
   "source": [
    "## Clustering Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be03d65",
   "metadata": {},
   "source": [
    "In this step, the Data_feas and Data_infeas databases are created to keep track of all points. From the Data_feas database take the reduced and scaled variables for the clustering step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2bec8fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_feas = Data_reg[Data_reg[:, 0] == 1][:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33f75fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_clust = Data_feas[:, 0]\n",
    "Vars_clust = Data_feas[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a067d7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_clust, X_test_clust, z_train_clust, z_test_clust = train_test_split(Vars_clust, z_clust, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d529d94",
   "metadata": {},
   "source": [
    "## Optimal Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "59c944d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_opt = 6.2625\n",
    "x2_opt = 4.3345\n",
    "x3_opt = 0\n",
    "z_opt = NLP_obj([x1_opt, x2_opt, x3_opt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "41b68607",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_opt = np.array([x1_opt, x2_opt, x3_opt]).reshape(1,-1)\n",
    "var_opt_scaled = scaler_std.transform(var_opt)\n",
    "var_opt_red = clf_red.transform(var_opt_scaled)\n",
    "z_opt_red = scaler_minmax.transform(np.array(z_opt).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a4ff75",
   "metadata": {},
   "source": [
    "# Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7dc172",
   "metadata": {},
   "source": [
    "In this section, a regression, clasification and clustering algorithms are trained and tested. \n",
    "The regression and clasification methods are trained with partial fit, and the clustering with Gaussian Mixtures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae617274",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "95d7c04a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SGDRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SGDRegressor</label><div class=\"sk-toggleable__content\"><pre>SGDRegressor()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SGDRegressor()"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SGDRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2d83ef7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_reg = SGDRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "328b6953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SGDRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SGDRegressor</label><div class=\"sk-toggleable__content\"><pre>SGDRegressor()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SGDRegressor()"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_reg.fit(X_train_reg, z_train_reg.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9fee961b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9901243181539634"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_reg.score(X_test_reg, z_test_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d7483d",
   "metadata": {},
   "source": [
    "## Clasification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d05184ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_class = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4e214854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GaussianNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianNB</label><div class=\"sk-toggleable__content\"><pre>GaussianNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_class.partial_fit(X_train_clas, y_train_clas, all_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "38fd41fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_class.score(X_test_clas, y_test_clas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488c8348",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ea5a50",
   "metadata": {},
   "source": [
    "A Gaussian mixture model (GMM) is a probabilistic model that assumes that the data is generated from a mixture of multiple Gaussian distributions. A Gaussian distribution, also known as a normal distribution, defined by two parameters: the mean (average) and the standard deviation.\n",
    "\n",
    "A GMM is a way of modeling data that is composed of multiple Gaussian distributions. In a GMM, each Gaussian distribution is called a component, and the mixture of these components represents the probability density function of the data. The GMM assigns a weight to each component, which represents the probability that a data point was generated by that componen. [ChatGPT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "17be29ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = GaussianMixture(n_components=1, covariance_type=\"full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7e18a524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GaussianMixture()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianMixture</label><div class=\"sk-toggleable__content\"><pre>GaussianMixture()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GaussianMixture()"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmm.fit(X_train_clust)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db978f74",
   "metadata": {},
   "source": [
    "# Search of optima"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4c6a48",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555bfc75",
   "metadata": {},
   "source": [
    "Since we have a GMM that assumes data is generated from a mixture of multiple Gaussian distributions, we can generate a confidence interval. A confidence region is a is a set of points in an n-dimensional space, often represented as an ellipsoid around a point satisfying\n",
    "\n",
    "$ (x - \\mu)^{T} \\,\\Sigma^{-1} \\, (x - \\mu) \\leq \\chi_{p, \\alpha}^{2} $\n",
    "\n",
    "The boundary of the set is described by the parametric curve\n",
    "\n",
    "$ x(\\theta) = \\mu + \\sqrt{(\\chi_{p, \\alpha}^{2})} \\, L \\, \\Theta $\n",
    "\n",
    "Where $L$ is the Cholesky factorization of $\\Sigma$, and $\\Theta$ is a vector of the parameters representation. For the bivariare case\n",
    "\n",
    "$ \\Theta = \\left[ \\begin{array}{c} \n",
    "cos \\theta \\\\\n",
    "sin \\theta\n",
    "\\end{array} \\right] \\, ; \\,  0 < \\theta < 2 \\pi $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b2094731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ellipse(mu, L, theta, n_std):\n",
    "    \n",
    "    xp = mu + np.sqrt(n_std)*np.dot(L,[np.cos(theta), np.sin(theta)])\n",
    "    \n",
    "    return xp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "274905bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_scaled(X, interpol):\n",
    "    \n",
    "    x_min = np.amin(X)\n",
    "    x_max = np.amax(X)\n",
    "    p_max = np.argmax(X)\n",
    "    \n",
    "    return (x_max - x_min)*interpol + x_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ed517b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_theta(Z, Theta):\n",
    "    \n",
    "    New_theta = []\n",
    "    A = np.hstack((Z, Theta))\n",
    "    zmim = np.amin(A[:,0])\n",
    "    zmax = np.amax(A[:,0])\n",
    "    pmax = np.argmax(A[:,0])\n",
    "    zint = (zmax-zmim)*0.75 + zmim\n",
    "    \n",
    "    if pmax == 0:\n",
    "        A_half = A[pmax:,:]\n",
    "        P_up = np.argmin(A_half[:,0])\n",
    "        P1 = A_half[P_up, :]\n",
    "        P2 = A[0,:]\n",
    "        Temp = (P2[1:] - P1[1:])*((zint-P1[0])/(P2[0]-P1[0])) + P1[1:]\n",
    "        New_theta.append(np.array(P2))\n",
    "        New_theta.append(Temp)\n",
    "    elif pmax == A.shape[0]:\n",
    "        A_half = A[:pmax+1,:]\n",
    "        P_down = np.argmax(A_half[:,pmax])\n",
    "        P1 = A[0,:]\n",
    "        P2 = A_half[P_down, :]\n",
    "        Temp = (P2[1:] - P1[1:])*((zint-P1[0])/(P2[0]-P1[0])) + P1[1:]\n",
    "        New_theta.append(Temp)\n",
    "        New_theta.append(np.array(P1))\n",
    "#        New_theta = np.array([Temp, np.array(P1)])\n",
    "    else:\n",
    "        for i in range(2):\n",
    "            if i == 0:\n",
    "                A_half = A[:pmax+1,:]\n",
    "            elif i == 1:\n",
    "                A_half = A[pmax:,:]\n",
    "\n",
    "            A_up = A_half[A_half[:,0] > zint]\n",
    "            if A_up.size == 0:\n",
    "                P_up = np.argmin(A_half[:,0])\n",
    "                P1 = A_half[P_up, :]\n",
    "            else:\n",
    "                P_up = np.argmin(A_up[:,0])\n",
    "                P1 = A_up[P_up, :]\n",
    "\n",
    "            A_down = A_half[A_half[:,0] < zint]\n",
    "            if A_down.size == 0:\n",
    "                P_down = np.argmax(A_half[:,0])\n",
    "                P2 = A_half[P_down, :]\n",
    "            else:\n",
    "                P_down = np.argmax(A_down[:,0])\n",
    "                P2 = A_down[P_down, :]\n",
    "            Temp = (P2[1:] - P1[1:])*((zint-P1[0])/(P2[0]-P1[0])) + P1[1:]\n",
    "            New_theta.append(Temp)\n",
    "    \n",
    "    return New_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71e7698",
   "metadata": {},
   "source": [
    "## Cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8c0d85fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for axis 1 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plt\u001b[39m.\u001b[39mscatter(Data_feas[:,\u001b[39m1\u001b[39m], Data_feas[:,\u001b[39m2\u001b[39;49m])\n\u001b[1;32m      2\u001b[0m plt\u001b[39m.\u001b[39mscatter(Data_infeas[:,\u001b[39m1\u001b[39m], Data_infeas[:,\u001b[39m2\u001b[39m])\n\u001b[1;32m      3\u001b[0m plt\u001b[39m.\u001b[39mscatter(var_opt_red[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m], var_opt_red[\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m], c\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mred\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for axis 1 with size 2"
     ]
    }
   ],
   "source": [
    "plt.scatter(Data_feas[:,1], Data_feas[:,2])\n",
    "plt.scatter(Data_infeas[:,1], Data_infeas[:,2])\n",
    "plt.scatter(var_opt_red[0][0], var_opt_red[0][1], c=\"red\")\n",
    "\n",
    "epoch_max = 5\n",
    "ite_max = 10\n",
    "\n",
    "for epoch in range(epoch_max):\n",
    "    \n",
    "    plt.figure()\n",
    "    \n",
    "    plt.scatter(Data_feas[:,1], Data_feas[:,2])\n",
    "    plt.scatter(Data_infeas[:,1], Data_infeas[:,2])\n",
    "    plt.scatter(var_opt_red[0][0], var_opt_red[0][1], c=\"red\")\n",
    "    \n",
    "    # Assign:\n",
    "    n_std = 1\n",
    "    flag_n_std_min = 1\n",
    "    flag_n_std_max = 1\n",
    "    flag_ite = 1\n",
    "    Data_chunk = []\n",
    "    \n",
    "    # Compute μ, Σ, and L\n",
    "    mu = gmm.means_[0]\n",
    "    sigma = gmm.covariances_\n",
    "    L = np.linalg.cholesky(sigma)\n",
    "    \n",
    "    # Compute: \n",
    "    z_max = max(Data_feas[:,0])\n",
    "    z_max_scaled = max_scaled(Data_feas[:,0], 0.95)\n",
    "    \n",
    "    for ite in range(ite_max):\n",
    "        \n",
    "        count = 0\n",
    "        \n",
    "        if flag_ite == 0:\n",
    "            flag_ite = 1\n",
    "            break\n",
    "            \n",
    "        if flag_n_std_min == 1:\n",
    "            \n",
    "            log_ll_clust = -gmm.score(Vars_clust)\n",
    "            \n",
    "            # Compute n_std_min\n",
    "            for i in range(21):\n",
    "                Data_std = [] \n",
    "\n",
    "                for theta in np.linspace(0, 2*np.pi, 30):\n",
    "                    Data_std.append(np.append(ellipse(mu, L, theta, n_std), [theta]))\n",
    "\n",
    "                Data_std = np.array(Data_std)\n",
    "                z_std = clf_reg.predict(Data_std[:, 0:dim_pca])\n",
    "\n",
    "                if -gmm.score(Data_std[:, 0:dim_pca]) > log_ll_clust or max(z_std) > z_max_scaled or i == 20:\n",
    "                    n_std_min = n_std\n",
    "                    break\n",
    "\n",
    "                n_std += 0.10\n",
    "\n",
    "            plt.plot(Data_std[:,0], Data_std[:,1], c='green')\n",
    "\n",
    "        if flag_n_std_max == 1:\n",
    "            \n",
    "            # Compute  n_std_max\n",
    "            for i in range(31):\n",
    "                Data_std = []\n",
    "\n",
    "                for theta in np.linspace(0, 2*np.pi, 30):\n",
    "                    Data_std.append(np.append(ellipse(mu, L, theta, n_std), [theta]))\n",
    "\n",
    "                Data_std = np.array(Data_std)\n",
    "                Data_std_feas = Data_std[clf_class.predict(Data_std[:, 0:dim_pca]) == 1]\n",
    "\n",
    "                if Data_std_feas.shape[0] == 0 or i == 30:\n",
    "                    n_std_max = n_std\n",
    "                    break\n",
    "\n",
    "            plt.plot(Data_std[:,0], Data_std[:,1], c='black')        \n",
    "\n",
    "        for n_std in np.linspace(n_std_min - 0.10, n_std_max, 10):\n",
    "            \n",
    "            count += 1\n",
    "\n",
    "            # Create ellipse given n_std\n",
    "            Data_std = []\n",
    "            \n",
    "            print(epoch, ite, count, n_std, n_std_min, n_std_max)\n",
    "\n",
    "            for theta in np.linspace(0, 2*np.pi, 30):\n",
    "                Data_std.append(np.append(ellipse(mu, L, theta, n_std), [theta]))\n",
    "\n",
    "            Data_std = np.array(Data_std)\n",
    "            \n",
    "            # Data_std_feas\n",
    "            Data_std_feas = Data_std[clf_class.predict(Data_std[:, 0:dim_pca]) == 1]\n",
    "\n",
    "            if Data_std_feas.shape[0] == 0:\n",
    "                print('Esc : No feasible sol ellipse')\n",
    "                n_std = n_std_min\n",
    "                n_std_min = n_std_min/10\n",
    "                flag_n_std_min = 0\n",
    "                break\n",
    "            \n",
    "            z_sample = clf_reg.predict(Data_std_feas[:, 0:dim_pca])\n",
    "            Angle_sample = Data_std_feas[:, dim_pca:]\n",
    "            \n",
    "            # New theta\n",
    "            if Angle_sample.size == 1:\n",
    "                New_theta = np.array([Angle_sample[0] - 0.1, Angle_sample[0] + 0.1])\n",
    "            elif Angle_sample.size == 2:\n",
    "                New_theta = Angle_sample\n",
    "            else:\n",
    "                New_theta = opt_theta(z_sample.reshape(-1,1), Angle_sample)\n",
    "\n",
    "            # Data_search\n",
    "            Data_search = []\n",
    "\n",
    "            for theta in np.linspace(New_theta[0][0], New_theta[1][0], 10):\n",
    "\n",
    "                Data_search.append(np.append(ellipse(mu, L, theta, n_std), [theta]))\n",
    "\n",
    "            Data_search = np.array(Data_search)\n",
    "            \n",
    "            # Data_search_feas\n",
    "            Data_search_feas = Data_search[clf_class.predict(Data_search[:, 0:dim_pca]) == 1]\n",
    "            Vars_search = Data_search_feas[:,0:dim_pca]\n",
    "            z_search = clf_reg.predict(Vars_search)\n",
    "\n",
    "            plt.scatter(Vars_search[:,0], Vars_search[:,1], c=\"brown\")\n",
    "            print(z_max, z_opt_red)\n",
    "\n",
    "            # Var_search_decom and Vars_search_decaled\n",
    "            Var_search_decom = clf_red.inverse_transform(Vars_search)\n",
    "            Vars_search_decaled = scaler_std.inverse_transform(Var_search_decom)\n",
    "            x1_new, x2_new, x3_new = Vars_search_decaled[:,0], Vars_search_decaled[:,1], Vars_search_decaled[:,2]\n",
    "            \n",
    "            # Data new\n",
    "            Data_new = []\n",
    "\n",
    "            for i in range(Vars_search.shape[0]):\n",
    "\n",
    "                feas, z, x1, x2, x3 = NLP(x1_new[i], x2_new[i], x3_new[i])\n",
    "                Data_new.append([feas, z, x1, x2, x3])\n",
    "\n",
    "            Data_new = np.array(Data_new)\n",
    "\n",
    "            # y_chunk\n",
    "            y_prep_new = Data_new[:, 0]\n",
    "            z_prep_new = Data_new[:, 1]\n",
    "            Vars_prep_new = Data_new[:, 2:]\n",
    "            Vars_scaled_new = scaler_std.transform(Vars_prep_new)\n",
    "            Vars_red_new = clf_red.transform(Vars_scaled_new)\n",
    "            \n",
    "            # Save results\n",
    "            if count == 1:\n",
    "                Data_chunk = np.hstack((y_prep_new.reshape(-1,1), z_prep_new.reshape(-1,1), Vars_red_new))\n",
    "            else:\n",
    "                Data_chunk = np.vstack((Data_chunk, np.hstack((y_prep_new.reshape(-1,1), z_prep_new.reshape(-1,1), Vars_red_new))))            \n",
    "            \n",
    "            # y_clas\n",
    "            y_clas_chunk = Data_chunk[:, 0]\n",
    "\n",
    "            if Data_new[y_prep_new == 1].shape[0] == 0:\n",
    "                if count == 1 and ite == 0:\n",
    "                    print('Esc: No feasible solution firts ite')\n",
    "                    n_std = n_std_min\n",
    "                    n_std_min = n_std/10\n",
    "                    flag_n_std_min = 0\n",
    "                    break\n",
    "                if Data_chunk[y_clas_chunk == 1].shape[0] == 0:\n",
    "                    print('Esc: No feasible solution Data chunck')\n",
    "                    n_std_max = n_std_min\n",
    "                    n_std_min = n_std_min/10\n",
    "                    flag_n_std_min = 0\n",
    "                    flag_n_std_max = 0\n",
    "                    break\n",
    "                else:\n",
    "                    print('Esc: No feasible solution Data_new but feasible for Data chunck')\n",
    "                    flag_ite = 0\n",
    "            else:\n",
    "                if ite < ite_max:\n",
    "                    flag_ite = 1\n",
    "                    print('Esc: Continue search')\n",
    "                else:\n",
    "                    print('Esc 4')\n",
    "                    flag_ite = 0\n",
    "\n",
    "            if flag_ite == 0:\n",
    "\n",
    "                # Data_clas_chunk\n",
    "                z_clas_chunk = Data_chunk[:, 1]\n",
    "                Vars_clas_chunk = Data_chunk[:, 2:]\n",
    "                X_train_clas_chunk, X_test_clas_chunk, y_train_clas_chunk, y_test_clas_chunk = train_test_split(Vars_clas_chunk, y_clas_chunk, test_size=0.25)\n",
    "                \n",
    "                # Data_reg_chunk\n",
    "                Data_intractable_chunk = Data_chunk[Data_chunk[:, 1] != sys.float_info.min]\n",
    "                y_reg_chunk = Data_intractable_chunk[:, 0]\n",
    "                z_reg_chunk = Data_intractable_chunk[:, 1]\n",
    "                Vars_reg_chunk = Data_intractable_chunk[:, 2:]\n",
    "                z_scaled_chunk = scaler_minmax.transform(np.array(z_reg_chunk).reshape(-1,1))\n",
    "                Data_reg_chunk = np.concatenate((y_reg_chunk.reshape(-1,1), z_scaled_chunk.reshape(-1,1), Vars_reg_chunk), axis=1)\n",
    "                X_train_reg_chunk, X_test_reg_chunk, z_train_reg_chunk, z_test_reg_chunk = train_test_split(Vars_reg_chunk, z_scaled_chunk, test_size=0.25)\n",
    "                \n",
    "                # Data_chunk_infeas\n",
    "                Data_chunk_infeas = Data_chunk[Data_chunk[:, 0] == 0][:, 1:]\n",
    "                Data_infeas = np.vstack((Data_infeas, Data_chunk_infeas))\n",
    "\n",
    "                # Data_chunk_feas\n",
    "                Data_chunk_feas = Data_reg_chunk[Data_reg_chunk[:, 0] == 1][:, 1:]\n",
    "                Data_feas = np.vstack((Data_feas, Data_chunk_feas))\n",
    "                \n",
    "                # Data_clus\n",
    "                Data_clus = Data_feas[Data_feas[:, 0].argsort()][int(Data_feas.shape[0]/2):, :]\n",
    "                z_clust = Data_clus[:, 0]\n",
    "                Vars_clust = Data_clus[:, 1:]\n",
    "                X_train_clust, X_test_clust, z_train_clust, z_test_clust = train_test_split(Vars_clust, z_clust, test_size=0.25)\n",
    "                \n",
    "                # Fit\n",
    "                clf_reg.partial_fit(X_train_reg_chunk, z_train_reg_chunk.ravel())\n",
    "                clf_class.partial_fit(X_train_clas_chunk, y_train_clas_chunk, all_classes)\n",
    "                gmm.fit(X_train_clust)\n",
    "        \n",
    "                flag_n_std_min = 1\n",
    "                flag_n_std_max = 1\n",
    "\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6084c4e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "7db5e65c1e2c661efecdc030e1f32524a0afbe67d346303a209105058f72aaf3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
